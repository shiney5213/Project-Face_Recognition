{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# insightFace_Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T14:05:20.127932Z",
     "start_time": "2020-07-21T14:05:14.413652Z"
    }
   },
   "outputs": [],
   "source": [
    "from easydict import EasyDict as edict\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torchvision import transforms as trans\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# from data.data_pipe import de_preprocess, get_train_loader, get_val_data\n",
    "# from model import Backbone, Arcface, MobileFaceNet, Am_softmax, l2_norm\n",
    "# from verifacation import evaluate\n",
    "# from utils import get_time, gen_plot, hflip_batch, separate_bn_paras\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from tensorboardX import SummaryWriter\n",
    "from matplotlib import pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "from PIL import Image\n",
    "from torchvision import transforms as trans\n",
    "import math\n",
    "import bcolz\n",
    "\n",
    "from torch.nn import Linear, Conv2d, BatchNorm1d, BatchNorm2d, PReLU, ReLU, Sigmoid, Dropout2d, Dropout, AvgPool2d, MaxPool2d, AdaptiveAvgPool2d, Sequential, Module, Parameter\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from collections import namedtuple\n",
    "import math\n",
    "import pdb\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.decomposition import PCA\n",
    "import sklearn\n",
    "from scipy import interpolate\n",
    "import datetime\n",
    "import mxnet as mx\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-17T05:12:45.306177Z",
     "start_time": "2020-07-17T05:12:45.303180Z"
    }
   },
   "source": [
    "### utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T14:05:54.716418Z",
     "start_time": "2020-07-21T14:05:54.694418Z"
    }
   },
   "outputs": [],
   "source": [
    "def separate_bn_paras(modules):\n",
    "    if not isinstance(modules, list):\n",
    "        modules = [*modules.modules()]\n",
    "    paras_only_bn = []\n",
    "    paras_wo_bn = []\n",
    "    for layer in modules:\n",
    "        if 'model' in str(layer.__class__):\n",
    "            continue\n",
    "        if 'container' in str(layer.__class__):\n",
    "            continue\n",
    "        else:\n",
    "            if 'batchnorm' in str(layer.__class__):\n",
    "                paras_only_bn.extend([*layer.parameters()])\n",
    "            else:\n",
    "                paras_wo_bn.extend([*layer.parameters()])\n",
    "    return paras_only_bn, paras_wo_bn\n",
    "\n",
    "def hflip_batch(imgs_tensor):\n",
    "    hfliped_imgs = torch.empty_like(imgs_tensor)\n",
    "    for i, img_ten in enumerate(imgs_tensor):\n",
    "        hfliped_imgs[i] = hflip(img_ten)\n",
    "    return hfliped_imgs\n",
    "\n",
    "def get_time():\n",
    "    return (str(datetime.now())[:-10]).replace(' ','-').replace(':','-')\n",
    "\n",
    "def gen_plot(fpr, tpr):\n",
    "    \"\"\"Create a pyplot plot and save to buffer.\"\"\"\n",
    "    plt.figure()\n",
    "    plt.xlabel(\"FPR\", fontsize=14)\n",
    "    plt.ylabel(\"TPR\", fontsize=14)\n",
    "    plt.title(\"ROC Curve\", fontsize=14)\n",
    "    plot = plt.plot(fpr, tpr, linewidth=2)\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='jpeg')\n",
    "    buf.seek(0)\n",
    "    plt.close()\n",
    "    return buf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data_pipe.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T14:09:44.594474Z",
     "start_time": "2020-07-21T14:09:44.566478Z"
    }
   },
   "outputs": [],
   "source": [
    "def de_preprocess(tensor):\n",
    "    return tensor*0.5 + 0.5\n",
    "    \n",
    "def get_train_dataset(imgs_folder):\n",
    "    train_transform = trans.Compose([\n",
    "        trans.RandomHorizontalFlip(),\n",
    "        trans.ToTensor(),\n",
    "        trans.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "    ])\n",
    "    ds = ImageFolder(imgs_folder, train_transform)\n",
    "    class_num = ds[-1][1] + 1\n",
    "    print('class_num', class_num)\n",
    "    return ds, class_num\n",
    "\n",
    "def get_train_loader(conf):\n",
    "    if conf.data_mode in ['ms1m', 'concat']:\n",
    "        ms1m_ds, ms1m_class_num = get_train_dataset(conf.ms1m_folder/'imgs')\n",
    "        print('ms1m loader generated')\n",
    "    if conf.data_mode in ['vgg', 'concat']:\n",
    "        vgg_ds, vgg_class_num = get_train_dataset(conf.vgg_folder/'imgs')\n",
    "        print('vgg loader generated')        \n",
    "    if conf.data_mode == 'vgg':\n",
    "        ds = vgg_ds\n",
    "        class_num = vgg_class_num\n",
    "    elif conf.data_mode == 'ms1m':\n",
    "        ds = ms1m_ds\n",
    "        class_num = ms1m_class_num\n",
    "    elif conf.data_mode == 'concat':\n",
    "        for i,(url,label) in enumerate(vgg_ds.imgs):\n",
    "            vgg_ds.imgs[i] = (url, label + ms1m_class_num)\n",
    "        ds = ConcatDataset([ms1m_ds,vgg_ds])\n",
    "        class_num = vgg_class_num + ms1m_class_num\n",
    "    elif conf.data_mode == 'emore':\n",
    "        ds, class_num = get_train_dataset(conf.emore_folder/'imgs')\n",
    "    elif conf.data_mode == 'small_vgg':\n",
    "        ds, class_num = get_train_dataset(conf.smallvgg_folder/'imgs')\n",
    "        \n",
    "    loader = DataLoader(ds, batch_size=conf.batch_size, shuffle=True, pin_memory=conf.pin_memory, num_workers=conf.num_workers)\n",
    "    return loader, class_num \n",
    "\n",
    "def get_val_data(data_path):\n",
    "#     agedb_30, agedb_30_issame = get_val_pair(data_path, 'agedb_30')\n",
    "#     cfp_fp, cfp_fp_issame = get_val_pair(data_path, 'cfp_fp')\n",
    "#     lfw, lfw_issame = get_val_pair(data_path, 'lfw')\n",
    "#     return agedb_30, cfp_fp, lfw, agedb_30_issame, cfp_fp_issame, lfw_issame\n",
    "#     lfw, lfw_issame = get_val_pair(data_path, 'lfw')\n",
    "    lfw, lfw_issame = get_val_pair(data_path, 'small_lfw')\n",
    "\n",
    "    return  lfw, lfw_issame\n",
    "\n",
    "def get_val_pair(path, name):\n",
    "    carray = bcolz.carray(rootdir = path/name, mode='r')\n",
    "    issame = np.load(path/'{}_list.npy'.format(name))\n",
    "    return carray, issame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T14:09:47.414476Z",
     "start_time": "2020-07-21T14:09:47.282478Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "##################################  Original Arcface Model #############################################################\n",
    "\n",
    "class Flatten(Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "def l2_norm(input,axis=1):\n",
    "    norm = torch.norm(input,2,axis,True)\n",
    "    output = torch.div(input, norm)\n",
    "    return output\n",
    "\n",
    "class SEModule(Module):\n",
    "    def __init__(self, channels, reduction):\n",
    "        super(SEModule, self).__init__()\n",
    "        self.avg_pool = AdaptiveAvgPool2d(1)\n",
    "        self.fc1 = Conv2d(\n",
    "            channels, channels // reduction, kernel_size=1, padding=0 ,bias=False)\n",
    "        self.relu = ReLU(inplace=True)\n",
    "        self.fc2 = Conv2d(\n",
    "            channels // reduction, channels, kernel_size=1, padding=0 ,bias=False)\n",
    "        self.sigmoid = Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        module_input = x\n",
    "        x = self.avg_pool(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return module_input * x\n",
    "\n",
    "class bottleneck_IR(Module):\n",
    "    def __init__(self, in_channel, depth, stride):\n",
    "        super(bottleneck_IR, self).__init__()\n",
    "        if in_channel == depth:\n",
    "            self.shortcut_layer = MaxPool2d(1, stride)\n",
    "        else:\n",
    "            self.shortcut_layer = Sequential(\n",
    "                Conv2d(in_channel, depth, (1, 1), stride ,bias=False), BatchNorm2d(depth))\n",
    "        self.res_layer = Sequential(\n",
    "            BatchNorm2d(in_channel),\n",
    "            Conv2d(in_channel, depth, (3, 3), (1, 1), 1 ,bias=False), PReLU(depth),\n",
    "            Conv2d(depth, depth, (3, 3), stride, 1 ,bias=False), BatchNorm2d(depth))\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = self.shortcut_layer(x)\n",
    "        res = self.res_layer(x)\n",
    "        return res + shortcut\n",
    "\n",
    "class bottleneck_IR_SE(Module):\n",
    "    def __init__(self, in_channel, depth, stride):\n",
    "        super(bottleneck_IR_SE, self).__init__()\n",
    "        if in_channel == depth:\n",
    "            self.shortcut_layer = MaxPool2d(1, stride)\n",
    "        else:\n",
    "            self.shortcut_layer = Sequential(\n",
    "                Conv2d(in_channel, depth, (1, 1), stride ,bias=False), \n",
    "                BatchNorm2d(depth))\n",
    "        self.res_layer = Sequential(\n",
    "            BatchNorm2d(in_channel),\n",
    "            Conv2d(in_channel, depth, (3,3), (1,1),1 ,bias=False),\n",
    "            PReLU(depth),\n",
    "            Conv2d(depth, depth, (3,3), stride, 1 ,bias=False),\n",
    "            BatchNorm2d(depth),\n",
    "            SEModule(depth,16)\n",
    "            )\n",
    "    def forward(self,x):\n",
    "        shortcut = self.shortcut_layer(x)\n",
    "        res = self.res_layer(x)\n",
    "        return res + shortcut\n",
    "\n",
    "class Bottleneck(namedtuple('Block', ['in_channel', 'depth', 'stride'])):\n",
    "    '''A named tuple describing a ResNet block.'''\n",
    "    \n",
    "def get_block(in_channel, depth, num_units, stride = 2):\n",
    "  return [Bottleneck(in_channel, depth, stride)] + [Bottleneck(depth, depth, 1) for i in range(num_units-1)]\n",
    "\n",
    "def get_blocks(num_layers):\n",
    "    if num_layers == 50:\n",
    "        blocks = [\n",
    "            get_block(in_channel=64, depth=64, num_units = 3),\n",
    "            get_block(in_channel=64, depth=128, num_units=4),\n",
    "            get_block(in_channel=128, depth=256, num_units=14),\n",
    "            get_block(in_channel=256, depth=512, num_units=3)\n",
    "        ]\n",
    "    elif num_layers == 100:\n",
    "        blocks = [\n",
    "            get_block(in_channel=64, depth=64, num_units=3),\n",
    "            get_block(in_channel=64, depth=128, num_units=13),\n",
    "            get_block(in_channel=128, depth=256, num_units=30),\n",
    "            get_block(in_channel=256, depth=512, num_units=3)\n",
    "        ]\n",
    "    elif num_layers == 152:\n",
    "        blocks = [\n",
    "            get_block(in_channel=64, depth=64, num_units=3),\n",
    "            get_block(in_channel=64, depth=128, num_units=8),\n",
    "            get_block(in_channel=128, depth=256, num_units=36),\n",
    "            get_block(in_channel=256, depth=512, num_units=3)\n",
    "        ]\n",
    "    return blocks\n",
    "\n",
    "class Backbone(Module):\n",
    "    def __init__(self, num_layers, drop_ratio, mode='ir'):\n",
    "        super(Backbone, self).__init__()\n",
    "        assert num_layers in [50, 100, 152], 'num_layers should be 50,100, or 152'\n",
    "        assert mode in ['ir', 'ir_se'], 'mode should be ir or ir_se'\n",
    "        blocks = get_blocks(num_layers)\n",
    "        if mode == 'ir':\n",
    "            unit_module = bottleneck_IR\n",
    "        elif mode == 'ir_se':\n",
    "            unit_module = bottleneck_IR_SE\n",
    "        self.input_layer = Sequential(Conv2d(3, 64, (3, 3), 1, 1 ,bias=False), \n",
    "                                      BatchNorm2d(64), \n",
    "                                      PReLU(64))\n",
    "        self.output_layer = Sequential(BatchNorm2d(512), \n",
    "                                       Dropout(drop_ratio),\n",
    "                                       Flatten(),\n",
    "                                       Linear(512 * 7 * 7, 512),\n",
    "                                       BatchNorm1d(512))\n",
    "        modules = []\n",
    "        for block in blocks:\n",
    "            for bottleneck in block:\n",
    "                modules.append(\n",
    "                    unit_module(bottleneck.in_channel,\n",
    "                                bottleneck.depth,\n",
    "                                bottleneck.stride))\n",
    "        self.body = Sequential(*modules)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.input_layer(x)\n",
    "        x = self.body(x)\n",
    "        x = self.output_layer(x)\n",
    "        return l2_norm(x)\n",
    "\n",
    "##################################  MobileFaceNet #############################################################\n",
    "    \n",
    "class Conv_block(Module):\n",
    "    def __init__(self, in_c, out_c, kernel=(1, 1), stride=(1, 1), padding=(0, 0), groups=1):\n",
    "        super(Conv_block, self).__init__()\n",
    "        self.conv = Conv2d(in_c, out_channels=out_c, kernel_size=kernel, groups=groups, stride=stride, padding=padding, bias=False)\n",
    "        self.bn = BatchNorm2d(out_c)\n",
    "        self.prelu = PReLU(out_c)\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.prelu(x)\n",
    "        return x\n",
    "\n",
    "class Linear_block(Module):\n",
    "    def __init__(self, in_c, out_c, kernel=(1, 1), stride=(1, 1), padding=(0, 0), groups=1):\n",
    "        super(Linear_block, self).__init__()\n",
    "        self.conv = Conv2d(in_c, out_channels=out_c, kernel_size=kernel, groups=groups, stride=stride, padding=padding, bias=False)\n",
    "        self.bn = BatchNorm2d(out_c)\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return x\n",
    "\n",
    "class Depth_Wise(Module):\n",
    "     def __init__(self, in_c, out_c, residual = False, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=1):\n",
    "        super(Depth_Wise, self).__init__()\n",
    "        self.conv = Conv_block(in_c, out_c=groups, kernel=(1, 1), padding=(0, 0), stride=(1, 1))\n",
    "        self.conv_dw = Conv_block(groups, groups, groups=groups, kernel=kernel, padding=padding, stride=stride)\n",
    "        self.project = Linear_block(groups, out_c, kernel=(1, 1), padding=(0, 0), stride=(1, 1))\n",
    "        self.residual = residual\n",
    "     def forward(self, x):\n",
    "        if self.residual:\n",
    "            short_cut = x\n",
    "        x = self.conv(x)\n",
    "        x = self.conv_dw(x)\n",
    "        x = self.project(x)\n",
    "        if self.residual:\n",
    "            output = short_cut + x\n",
    "        else:\n",
    "            output = x\n",
    "        return output\n",
    "\n",
    "class Residual(Module):\n",
    "    def __init__(self, c, num_block, groups, kernel=(3, 3), stride=(1, 1), padding=(1, 1)):\n",
    "        super(Residual, self).__init__()\n",
    "        modules = []\n",
    "        for _ in range(num_block):\n",
    "            modules.append(Depth_Wise(c, c, residual=True, kernel=kernel, padding=padding, stride=stride, groups=groups))\n",
    "        self.model = Sequential(*modules)\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class MobileFaceNet(Module):\n",
    "    def __init__(self, embedding_size):\n",
    "        super(MobileFaceNet, self).__init__()\n",
    "        self.conv1 = Conv_block(3, 64, kernel=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.conv2_dw = Conv_block(64, 64, kernel=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
    "        self.conv_23 = Depth_Wise(64, 64, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=128)\n",
    "        self.conv_3 = Residual(64, num_block=4, groups=128, kernel=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.conv_34 = Depth_Wise(64, 128, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)\n",
    "        self.conv_4 = Residual(128, num_block=6, groups=256, kernel=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.conv_45 = Depth_Wise(128, 128, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=512)\n",
    "        self.conv_5 = Residual(128, num_block=2, groups=256, kernel=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.conv_6_sep = Conv_block(128, 512, kernel=(1, 1), stride=(1, 1), padding=(0, 0))\n",
    "        self.conv_6_dw = Linear_block(512, 512, groups=512, kernel=(7,7), stride=(1, 1), padding=(0, 0))\n",
    "        self.conv_6_flatten = Flatten()\n",
    "        self.linear = Linear(512, embedding_size, bias=False)\n",
    "        self.bn = BatchNorm1d(embedding_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "\n",
    "        out = self.conv2_dw(out)\n",
    "\n",
    "        out = self.conv_23(out)\n",
    "\n",
    "        out = self.conv_3(out)\n",
    "        \n",
    "        out = self.conv_34(out)\n",
    "\n",
    "        out = self.conv_4(out)\n",
    "\n",
    "        out = self.conv_45(out)\n",
    "\n",
    "        out = self.conv_5(out)\n",
    "\n",
    "        out = self.conv_6_sep(out)\n",
    "\n",
    "        out = self.conv_6_dw(out)\n",
    "\n",
    "        out = self.conv_6_flatten(out)\n",
    "\n",
    "        out = self.linear(out)\n",
    "\n",
    "        out = self.bn(out)\n",
    "        return l2_norm(out)\n",
    "\n",
    "##################################  Arcface head #############################################################\n",
    "\n",
    "class Arcface(Module):\n",
    "    # implementation of additive margin softmax loss in https://arxiv.org/abs/1801.05599    \n",
    "    def __init__(self, embedding_size=512, classnum=51332,  s=64., m=0.5):\n",
    "        super(Arcface, self).__init__()\n",
    "        self.classnum = classnum\n",
    "        self.kernel = Parameter(torch.Tensor(embedding_size,classnum))\n",
    "        # initial kernel\n",
    "        self.kernel.data.uniform_(-1, 1).renorm_(2,1,1e-5).mul_(1e5)\n",
    "        self.m = m # the margin value, default is 0.5\n",
    "        self.s = s # scalar value default is 64, see normface https://arxiv.org/abs/1704.06369\n",
    "        self.cos_m = math.cos(m)\n",
    "        self.sin_m = math.sin(m)\n",
    "        self.mm = self.sin_m * m  # issue 1\n",
    "        self.threshold = math.cos(math.pi - m)\n",
    "    def forward(self, embbedings, label):\n",
    "        # weights norm\n",
    "        nB = len(embbedings)\n",
    "        kernel_norm = l2_norm(self.kernel,axis=0)\n",
    "        # cos(theta+m)\n",
    "        cos_theta = torch.mm(embbedings,kernel_norm)\n",
    "#         output = torch.mm(embbedings,kernel_norm)\n",
    "        cos_theta = cos_theta.clamp(-1,1) # for numerical stability\n",
    "        cos_theta_2 = torch.pow(cos_theta, 2)\n",
    "        sin_theta_2 = 1 - cos_theta_2\n",
    "        sin_theta = torch.sqrt(sin_theta_2)\n",
    "        cos_theta_m = (cos_theta * self.cos_m - sin_theta * self.sin_m)\n",
    "        # this condition controls the theta+m should in range [0, pi]\n",
    "        #      0<=theta+m<=pi\n",
    "        #     -m<=theta<=pi-m\n",
    "        cond_v = cos_theta - self.threshold\n",
    "        cond_mask = cond_v <= 0\n",
    "        keep_val = (cos_theta - self.mm) # when theta not in [0,pi], use cosface instead\n",
    "        cos_theta_m[cond_mask] = keep_val[cond_mask]\n",
    "        output = cos_theta * 1.0 # a little bit hacky way to prevent in_place operation on cos_theta\n",
    "        idx_ = torch.arange(0, nB, dtype=torch.long)\n",
    "        output[idx_, label] = cos_theta_m[idx_, label]\n",
    "        output *= self.s # scale up in order to make softmax work, first introduced in normface\n",
    "        return output\n",
    "\n",
    "##################################  Cosface head #############################################################    \n",
    "    \n",
    "class Am_softmax(Module):\n",
    "    # implementation of additive margin softmax loss in https://arxiv.org/abs/1801.05599    \n",
    "    def __init__(self,embedding_size=512,classnum=51332):\n",
    "        super(Am_softmax, self).__init__()\n",
    "        self.classnum = classnum\n",
    "        self.kernel = Parameter(torch.Tensor(embedding_size,classnum))\n",
    "        # initial kernel\n",
    "        self.kernel.data.uniform_(-1, 1).renorm_(2,1,1e-5).mul_(1e5)\n",
    "        self.m = 0.35 # additive margin recommended by the paper\n",
    "        self.s = 30. # see normface https://arxiv.org/abs/1704.06369\n",
    "    def forward(self,embbedings,label):\n",
    "        kernel_norm = l2_norm(self.kernel,axis=0)\n",
    "        cos_theta = torch.mm(embbedings,kernel_norm)\n",
    "        cos_theta = cos_theta.clamp(-1,1) # for numerical stability\n",
    "        phi = cos_theta - self.m\n",
    "        label = label.view(-1,1) #size=(B,1)\n",
    "        index = cos_theta.data * 0.0 #size=(B,Classnum)\n",
    "        index.scatter_(1,label.data.view(-1,1),1)\n",
    "        index = index.byte()\n",
    "        output = cos_theta * 1.0\n",
    "        output[index] = phi[index] #only change the correct predicted output\n",
    "        output *= self.s # scale up in order to make softmax work, first introduced in normface\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### verifications.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T12:15:48.961607Z",
     "start_time": "2020-07-21T12:15:48.920605Z"
    }
   },
   "source": [
    "def evaluate(embeddings, actual_issame, nrof_folds=10, pca=0):\n",
    "    # Calculate evaluation metrics\n",
    "    print('embeddings.shape',embeddings.shape)\n",
    "    thresholds = np.arange(0, 4, 0.01)\n",
    "    embeddings1 = embeddings[0::2]\n",
    "    embeddings2 = embeddings[1::2]\n",
    "    tpr, fpr, accuracy, best_thresholds = calculate_roc(thresholds, embeddings1, embeddings2,\n",
    "                                       np.asarray(actual_issame), nrof_folds=nrof_folds, pca=pca)\n",
    "#     thresholds = np.arange(0, 4, 0.001)\n",
    "#     val, val_std, far = calculate_val(thresholds, embeddings1, embeddings2,\n",
    "#                                       np.asarray(actual_issame), 1e-3, nrof_folds=nrof_folds)\n",
    "#     return tpr, fpr, accuracy, best_thresholds, val, val_std, far\n",
    "    return tpr, fpr, accuracy, best_thresholds\n",
    "\n",
    "def calculate_roc(thresholds, embeddings1, embeddings2, actual_issame, nrof_folds=10, pca=0):\n",
    "    assert (embeddings1.shape[0] == embeddings2.shape[0])\n",
    "    assert (embeddings1.shape[1] == embeddings2.shape[1])\n",
    "    nrof_pairs = min(len(actual_issame), embeddings1.shape[0])\n",
    "    print('nrof_pairs',nrof_pairs)\n",
    "    nrof_thresholds = len(thresholds)\n",
    "    k_fold = KFold(n_splits=nrof_folds, shuffle=False)\n",
    "\n",
    "    tprs = np.zeros((nrof_folds, nrof_thresholds))\n",
    "    fprs = np.zeros((nrof_folds, nrof_thresholds))\n",
    "    accuracy = np.zeros((nrof_folds))\n",
    "    best_thresholds = np.zeros((nrof_folds))\n",
    "    indices = np.arange(nrof_pairs)\n",
    "    # print('pca', pca)\n",
    "\n",
    "    if pca == 0:\n",
    "        diff = np.subtract(embeddings1, embeddings2)\n",
    "        dist = np.sum(np.square(diff), 1)\n",
    "\n",
    "    for fold_idx, (train_set, test_set) in enumerate(k_fold.split(indices)):\n",
    "        # print('train_set', train_set)\n",
    "        # print('test_set', test_set)\n",
    "        if pca > 0:\n",
    "            print('doing pca on', fold_idx)\n",
    "            embed1_train = embeddings1[train_set]\n",
    "            embed2_train = embeddings2[train_set]\n",
    "            _embed_train = np.concatenate((embed1_train, embed2_train), axis=0)\n",
    "            # print(_embed_train.shape)\n",
    "            pca_model = PCA(n_components=pca)\n",
    "            pca_model.fit(_embed_train)\n",
    "            embed1 = pca_model.transform(embeddings1)\n",
    "            embed2 = pca_model.transform(embeddings2)\n",
    "            embed1 = sklearn.preprocessing.normalize(embed1)\n",
    "            embed2 = sklearn.preprocessing.normalize(embed2)\n",
    "            # print(embed1.shape, embed2.shape)\n",
    "            diff = np.subtract(embed1, embed2)\n",
    "            dist = np.sum(np.square(diff), 1)\n",
    "\n",
    "        # Find the best threshold for the fold\n",
    "        acc_train = np.zeros((nrof_thresholds))\n",
    "        for threshold_idx, threshold in enumerate(thresholds):\n",
    "            _, _, acc_train[threshold_idx] = calculate_accuracy(threshold, dist[train_set], actual_issame[train_set])\n",
    "        best_threshold_index = np.argmax(acc_train)\n",
    "#         print('best_threshold_index', best_threshold_index, acc_train[best_threshold_index])\n",
    "        best_thresholds[fold_idx] = thresholds[best_threshold_index]\n",
    "        for threshold_idx, threshold in enumerate(thresholds):\n",
    "            tprs[fold_idx, threshold_idx], fprs[fold_idx, threshold_idx], _ = calculate_accuracy(threshold,\n",
    "                                                                                                 dist[test_set],\n",
    "                                                                                                 actual_issame[\n",
    "                                                                                                     test_set])\n",
    "        _, _, accuracy[fold_idx] = calculate_accuracy(thresholds[best_threshold_index], dist[test_set],\n",
    "                                                      actual_issame[test_set])\n",
    "\n",
    "    tpr = np.mean(tprs, 0)\n",
    "    fpr = np.mean(fprs, 0)\n",
    "    return tpr, fpr, accuracy, best_thresholds\n",
    "\n",
    "def calculate_accuracy(threshold, dist, actual_issame):\n",
    "    predict_issame = np.less(dist, threshold)\n",
    "    tp = np.sum(np.logical_and(predict_issame, actual_issame))\n",
    "    fp = np.sum(np.logical_and(predict_issame, np.logical_not(actual_issame)))\n",
    "    tn = np.sum(np.logical_and(np.logical_not(predict_issame), np.logical_not(actual_issame)))\n",
    "    fn = np.sum(np.logical_and(np.logical_not(predict_issame), actual_issame))\n",
    "\n",
    "    tpr = 0 if (tp + fn == 0) else float(tp) / float(tp + fn)\n",
    "    fpr = 0 if (fp + tn == 0) else float(fp) / float(fp + tn)\n",
    "    acc = float(tp + tn) / dist.size\n",
    "    return tpr, fpr, acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learner.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T14:09:48.109477Z",
     "start_time": "2020-07-21T14:09:48.029476Z"
    }
   },
   "outputs": [],
   "source": [
    "class face_learner(object):\n",
    "    def __init__(self, conf, inference=False):\n",
    "        print(conf)\n",
    "        if conf.use_mobilfacenet:\n",
    "            self.model = MobileFaceNet(conf.embedding_size).to(conf.device)\n",
    "            print('MobileFaceNet model generated')\n",
    "        else:\n",
    "            self.model = Backbone(conf.net_depth, conf.drop_ratio, conf.net_mode).to(conf.device)\n",
    "            print('{}_{} model generated'.format(conf.net_mode, conf.net_depth))\n",
    "        \n",
    "        \n",
    "        if not inference:\n",
    "            self.milestones = conf.milestones\n",
    "            self.loader, self.class_num = get_train_loader(conf)   \n",
    "            print('self.loader',self.loader)\n",
    "\n",
    "            self.writer = SummaryWriter(conf.log_path)\n",
    "            self.step = 0\n",
    "            self.head = Arcface(embedding_size=conf.embedding_size, classnum=self.class_num).to(conf.device)\n",
    "\n",
    "            print('two model heads generated')\n",
    "\n",
    "            paras_only_bn, paras_wo_bn = separate_bn_paras(self.model)\n",
    "        \n",
    "            \n",
    "            if conf.use_mobilfacenet:\n",
    "                self.optimizer = optim.SGD([\n",
    "                                    {'params': paras_wo_bn[:-1], 'weight_decay': 4e-5},\n",
    "                                    {'params': [paras_wo_bn[-1]] + [self.head.kernel], 'weight_decay': 4e-4},\n",
    "                                    {'params': paras_only_bn}\n",
    "                                ], lr = conf.lr, momentum = conf.momentum)\n",
    "            else:\n",
    "                self.optimizer = optim.SGD([\n",
    "                                    {'params': paras_wo_bn + [self.head.kernel], 'weight_decay': 5e-4},\n",
    "#                                     {'params': paras_only_bn}\n",
    "                                ], lr = conf.lr, momentum = conf.momentum)\n",
    "            print(self.optimizer)\n",
    "#             self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, patience=40, verbose=True)\n",
    "\n",
    "            print('optimizers generated')    \n",
    "#             self.board_loss_every = len(self.loader)//100\n",
    "#             self.evaluate_every = len(self.loader)//10\n",
    "#             self.save_every = len(self.loader)//5\n",
    "           \n",
    "            self.board_loss_every = len(self.loader)\n",
    "            self.evaluate_every = len(self.loader)\n",
    "            self.save_every = len(self.loader)\n",
    "#             self.agedb_30, self.cfp_fp, self.lfw, self.agedb_30_issame, self.cfp_fp_issame, self.lfw_issame = get_val_data(self.loader.dataset.root.parent)\n",
    "            self.lfw, self.lfw_issame = get_val_data(conf.smallvgg_folder)\n",
    "\n",
    "#         else:\n",
    "            self.threshold = conf.threshold\n",
    "    \n",
    "    def save_state(self, conf, accuracy, to_save_folder=False, extra=None, model_only=False):\n",
    "        if to_save_folder:\n",
    "            save_path = conf.save_path\n",
    "        else:\n",
    "            save_path = conf.model_path\n",
    "        torch.save(\n",
    "            self.model.state_dict(), save_path /\n",
    "            ('model_{}_accuracy:{}_step:{}_{}.pth'.format(get_time(), accuracy, self.step, extra)))\n",
    "        if not model_only:\n",
    "            torch.save(\n",
    "                self.head.state_dict(), save_path /\n",
    "                ('head_{}_accuracy:{}_step:{}_{}.pth'.format(get_time(), accuracy, self.step, extra)))\n",
    "            torch.save(\n",
    "                self.optimizer.state_dict(), save_path /\n",
    "                ('optimizer_{}_accuracy:{}_step:{}_{}.pth'.format(get_time(), accuracy, self.step, extra)))\n",
    "    \n",
    "    def load_state(self, conf, fixed_str, from_save_folder=False, model_only=False):\n",
    "        if from_save_folder:\n",
    "            save_path = conf.save_path\n",
    "        else:\n",
    "            save_path = conf.model_path            \n",
    "        self.model.load_state_dict(torch.load(save_path/'model_{}'.format(fixed_str)))\n",
    "        if not model_only:\n",
    "            self.head.load_state_dict(torch.load(save_path/'head_{}'.format(fixed_str)))\n",
    "            self.optimizer.load_state_dict(torch.load(save_path/'optimizer_{}'.format(fixed_str)))\n",
    "        \n",
    "    def board_val(self, db_name, accuracy, best_threshold, roc_curve_tensor):\n",
    "        self.writer.add_scalar('{}_accuracy'.format(db_name), accuracy, self.step)\n",
    "        self.writer.add_scalar('{}_best_threshold'.format(db_name), best_threshold, self.step)\n",
    "        self.writer.add_image('{}_roc_curve'.format(db_name), roc_curve_tensor, self.step)\n",
    "#         self.writer.add_scalar('{}_val:true accept ratio'.format(db_name), val, self.step)\n",
    "#         self.writer.add_scalar('{}_val_std'.format(db_name), val_std, self.step)\n",
    "#         self.writer.add_scalar('{}_far:False Acceptance Ratio'.format(db_name), far, self.step)\n",
    "        \n",
    "    def evaluate(self, conf, carray, issame, nrof_folds = 5, tta = False):\n",
    "        self.model.eval()\n",
    "        idx = 0\n",
    "        embeddings = np.zeros([len(carray), conf.embedding_size])\n",
    "        with torch.no_grad():\n",
    "            while idx + conf.batch_size <= len(carray):\n",
    "                batch = torch.tensor(carray[idx:idx + conf.batch_size])\n",
    "                if tta:\n",
    "                    fliped = hflip_batch(batch)\n",
    "                    emb_batch = self.model(batch.to(conf.device)) + self.model(fliped.to(conf.device))\n",
    "                    embeddings[idx:idx + conf.batch_size] = l2_norm(emb_batch)\n",
    "                else:\n",
    "                    embeddings[idx:idx + conf.batch_size] = self.model(batch.to(conf.device)).cpu()\n",
    "                idx += conf.batch_size\n",
    "            if idx < len(carray):\n",
    "                batch = torch.tensor(carray[idx:])            \n",
    "                if tta:\n",
    "                    fliped = hflip_batch(batch)\n",
    "                    emb_batch = self.model(batch.to(conf.device)) + self.model(fliped.to(conf.device))\n",
    "                    embeddings[idx:] = l2_norm(emb_batch)\n",
    "                else:\n",
    "                    embeddings[idx:] = self.model(batch.to(conf.device)).cpu()\n",
    "        tpr, fpr, accuracy, best_thresholds = evaluate(embeddings, issame, nrof_folds)\n",
    "        buf = gen_plot(fpr, tpr)\n",
    "        roc_curve = Image.open(buf)\n",
    "        roc_curve_tensor = trans.ToTensor()(roc_curve)\n",
    "        return accuracy.mean(), best_thresholds.mean(), roc_curve_tensor\n",
    "    \n",
    "    def find_lr(self,\n",
    "                conf,\n",
    "                init_value=1e-8,\n",
    "                final_value=10.,\n",
    "                beta=0.98,\n",
    "                bloding_scale=3.,\n",
    "                num=None):\n",
    "        if not num:\n",
    "            num = len(self.loader)\n",
    "        mult = (final_value / init_value)**(1 / num)\n",
    "        lr = init_value\n",
    "        for params in self.optimizer.param_groups:\n",
    "            params['lr'] = lr\n",
    "        self.model.train()\n",
    "        avg_loss = 0.\n",
    "        best_loss = 0.\n",
    "        batch_num = 0\n",
    "        losses = []\n",
    "        log_lrs = []\n",
    "        for i, (imgs, labels) in tqdm(enumerate(self.loader), total=num):\n",
    "\n",
    "            imgs = imgs.to(conf.device)\n",
    "            labels = labels.to(conf.device)\n",
    "            batch_num += 1          \n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            embeddings = self.model(imgs)\n",
    "            thetas = self.head(embeddings, labels)\n",
    "            loss = conf.ce_loss(thetas, labels)          \n",
    "          \n",
    "            #Compute the smoothed loss\n",
    "            avg_loss = beta * avg_loss + (1 - beta) * loss.item()\n",
    "            self.writer.add_scalar('avg_loss', avg_loss, batch_num)\n",
    "            smoothed_loss = avg_loss / (1 - beta**batch_num)\n",
    "            self.writer.add_scalar('smoothed_loss', smoothed_loss,batch_num)\n",
    "            #Stop if the loss is exploding\n",
    "            if batch_num > 1 and smoothed_loss > bloding_scale * best_loss:\n",
    "                print('exited with best_loss at {}'.format(best_loss))\n",
    "                plt.plot(log_lrs[10:-5], losses[10:-5])\n",
    "                return log_lrs, losses\n",
    "            #Record the best loss\n",
    "            if smoothed_loss < best_loss or batch_num == 1:\n",
    "                best_loss = smoothed_loss\n",
    "            #Store the values\n",
    "            losses.append(smoothed_loss)\n",
    "            log_lrs.append(math.log10(lr))\n",
    "            self.writer.add_scalar('log_lr', math.log10(lr), batch_num)\n",
    "            #Do the SGD step\n",
    "            #Update the lr for the next step\n",
    "\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            lr *= mult\n",
    "            for params in self.optimizer.param_groups:\n",
    "                params['lr'] = lr\n",
    "            if batch_num > num:\n",
    "                plt.plot(log_lrs[10:-5], losses[10:-5])\n",
    "                return log_lrs, losses    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T14:09:48.545477Z",
     "start_time": "2020-07-21T14:09:48.431479Z"
    }
   },
   "outputs": [],
   "source": [
    "class face_learner(object):\n",
    "    def __init__(self, conf, inference=False):\n",
    "        print(conf)\n",
    "        if conf.use_mobilfacenet:\n",
    "            self.model = MobileFaceNet(conf.embedding_size).to(conf.device)\n",
    "            print('MobileFaceNet model generated')\n",
    "        else:\n",
    "            self.model = Backbone(conf.net_depth, conf.drop_ratio, conf.net_mode).to(conf.device)\n",
    "            print('{}_{} model generated'.format(conf.net_mode, conf.net_depth))\n",
    "        \n",
    "        \n",
    "        if not inference:\n",
    "            self.milestones = conf.milestones\n",
    "            self.loader, self.class_num = get_train_loader(conf)   \n",
    "            print('self.loader',self.loader)\n",
    "\n",
    "            self.writer = SummaryWriter(conf.log_path)\n",
    "            self.step = 0\n",
    "            self.head = Arcface(embedding_size=conf.embedding_size, classnum=self.class_num).to(conf.device)\n",
    "\n",
    "            print('two model heads generated')\n",
    "\n",
    "            paras_only_bn, paras_wo_bn = separate_bn_paras(self.model)\n",
    "        \n",
    "            \n",
    "            if conf.use_mobilfacenet:\n",
    "                self.optimizer = optim.SGD([\n",
    "                                    {'params': paras_wo_bn[:-1], 'weight_decay': 4e-5},\n",
    "                                    {'params': [paras_wo_bn[-1]] + [self.head.kernel], 'weight_decay': 4e-4},\n",
    "                                    {'params': paras_only_bn}\n",
    "                                ], lr = conf.lr, momentum = conf.momentum)\n",
    "            else:\n",
    "                self.optimizer = optim.SGD([\n",
    "                                    {'params': paras_wo_bn + [self.head.kernel], 'weight_decay': 5e-4},\n",
    "#                                     {'params': paras_only_bn}\n",
    "                                ], lr = conf.lr, momentum = conf.momentum)\n",
    "            print(self.optimizer)\n",
    "#             self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, patience=40, verbose=True)\n",
    "\n",
    "            print('optimizers generated')    \n",
    "#             self.board_loss_every = len(self.loader)//100\n",
    "#             self.evaluate_every = len(self.loader)//10\n",
    "#             self.save_every = len(self.loader)//5\n",
    "           \n",
    "            self.board_loss_every = len(self.loader)\n",
    "            self.evaluate_every = len(self.loader)\n",
    "            self.save_every = len(self.loader)\n",
    "#             self.agedb_30, self.cfp_fp, self.lfw, self.agedb_30_issame, self.cfp_fp_issame, self.lfw_issame = get_val_data(self.loader.dataset.root.parent)\n",
    "            self.lfw, self.lfw_issame = get_val_data(conf.smallvgg_folder)\n",
    "\n",
    "#         else:\n",
    "            self.threshold = conf.threshold\n",
    "    \n",
    "    def save_state(self, conf, accuracy, to_save_folder=False, extra=None, model_only=False):\n",
    "        if to_save_folder:\n",
    "            save_path = conf.save_path\n",
    "        else:\n",
    "            save_path = conf.model_path\n",
    "        torch.save(\n",
    "            self.model.state_dict(), save_path /\n",
    "            ('model_{}_accuracy:{}_step:{}_{}.pth'.format(get_time(), accuracy, self.step, extra)))\n",
    "        if not model_only:\n",
    "            torch.save(\n",
    "                self.head.state_dict(), save_path /\n",
    "                ('head_{}_accuracy:{}_step:{}_{}.pth'.format(get_time(), accuracy, self.step, extra)))\n",
    "            torch.save(\n",
    "                self.optimizer.state_dict(), save_path /\n",
    "                ('optimizer_{}_accuracy:{}_step:{}_{}.pth'.format(get_time(), accuracy, self.step, extra)))\n",
    "    \n",
    "    def load_state(self, conf, fixed_str, from_save_folder=False, model_only=False):\n",
    "        if from_save_folder:\n",
    "            save_path = conf.save_path\n",
    "        else:\n",
    "            save_path = conf.model_path            \n",
    "        self.model.load_state_dict(torch.load(save_path/'model_{}'.format(fixed_str)))\n",
    "        if not model_only:\n",
    "            self.head.load_state_dict(torch.load(save_path/'head_{}'.format(fixed_str)))\n",
    "            self.optimizer.load_state_dict(torch.load(save_path/'optimizer_{}'.format(fixed_str)))\n",
    "        \n",
    "    def board_val(self, db_name, accuracy, best_threshold, roc_curve_tensor):\n",
    "        self.writer.add_scalar('{}_accuracy'.format(db_name), accuracy, self.step)\n",
    "        self.writer.add_scalar('{}_best_threshold'.format(db_name), best_threshold, self.step)\n",
    "        self.writer.add_image('{}_roc_curve'.format(db_name), roc_curve_tensor, self.step)\n",
    "#         self.writer.add_scalar('{}_val:true accept ratio'.format(db_name), val, self.step)\n",
    "#         self.writer.add_scalar('{}_val_std'.format(db_name), val_std, self.step)\n",
    "#         self.writer.add_scalar('{}_far:False Acceptance Ratio'.format(db_name), far, self.step)\n",
    "        \n",
    "    def evaluate(self, conf, carray, issame, nrof_folds = 5, tta = False):\n",
    "        self.model.eval()\n",
    "        idx = 0\n",
    "        embeddings = np.zeros([len(carray), conf.embedding_size])\n",
    "        with torch.no_grad():\n",
    "            while idx + conf.batch_size <= len(carray):\n",
    "                batch = torch.tensor(carray[idx:idx + conf.batch_size])\n",
    "                if tta:\n",
    "                    fliped = hflip_batch(batch)\n",
    "                    emb_batch = self.model(batch.to(conf.device)) + self.model(fliped.to(conf.device))\n",
    "                    embeddings[idx:idx + conf.batch_size] = l2_norm(emb_batch)\n",
    "                else:\n",
    "                    embeddings[idx:idx + conf.batch_size] = self.model(batch.to(conf.device)).cpu()\n",
    "                idx += conf.batch_size\n",
    "            if idx < len(carray):\n",
    "                batch = torch.tensor(carray[idx:])            \n",
    "                if tta:\n",
    "                    fliped = hflip_batch(batch)\n",
    "                    emb_batch = self.model(batch.to(conf.device)) + self.model(fliped.to(conf.device))\n",
    "                    embeddings[idx:] = l2_norm(emb_batch)\n",
    "                else:\n",
    "                    embeddings[idx:] = self.model(batch.to(conf.device)).cpu()\n",
    "        tpr, fpr, accuracy, best_thresholds = evaluate(embeddings, issame, nrof_folds)\n",
    "        buf = gen_plot(fpr, tpr)\n",
    "        roc_curve = Image.open(buf)\n",
    "        roc_curve_tensor = trans.ToTensor()(roc_curve)\n",
    "        return accuracy.mean(), best_thresholds.mean(), roc_curve_tensor\n",
    "    \n",
    "    def find_lr(self,\n",
    "                conf,\n",
    "                init_value=1e-8,\n",
    "                final_value=10.,\n",
    "                beta=0.98,\n",
    "                bloding_scale=3.,\n",
    "                num=None):\n",
    "        if not num:\n",
    "            num = len(self.loader)\n",
    "        mult = (final_value / init_value)**(1 / num)\n",
    "        lr = init_value\n",
    "        for params in self.optimizer.param_groups:\n",
    "            params['lr'] = lr\n",
    "        self.model.train()\n",
    "        avg_loss = 0.\n",
    "        best_loss = 0.\n",
    "        batch_num = 0\n",
    "        losses = []\n",
    "        log_lrs = []\n",
    "        for i, (imgs, labels) in tqdm(enumerate(self.loader), total=num):\n",
    "\n",
    "            imgs = imgs.to(conf.device)\n",
    "            labels = labels.to(conf.device)\n",
    "            batch_num += 1          \n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            embeddings = self.model(imgs)\n",
    "            thetas = self.head(embeddings, labels)\n",
    "            loss = conf.ce_loss(thetas, labels)          \n",
    "          \n",
    "            #Compute the smoothed loss\n",
    "            avg_loss = beta * avg_loss + (1 - beta) * loss.item()\n",
    "            self.writer.add_scalar('avg_loss', avg_loss, batch_num)\n",
    "            smoothed_loss = avg_loss / (1 - beta**batch_num)\n",
    "            self.writer.add_scalar('smoothed_loss', smoothed_loss,batch_num)\n",
    "            #Stop if the loss is exploding\n",
    "            if batch_num > 1 and smoothed_loss > bloding_scale * best_loss:\n",
    "                print('exited with best_loss at {}'.format(best_loss))\n",
    "                plt.plot(log_lrs[10:-5], losses[10:-5])\n",
    "                return log_lrs, losses\n",
    "            #Record the best loss\n",
    "            if smoothed_loss < best_loss or batch_num == 1:\n",
    "                best_loss = smoothed_loss\n",
    "            #Store the values\n",
    "            losses.append(smoothed_loss)\n",
    "            log_lrs.append(math.log10(lr))\n",
    "            self.writer.add_scalar('log_lr', math.log10(lr), batch_num)\n",
    "            #Do the SGD step\n",
    "            #Update the lr for the next step\n",
    "\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            lr *= mult\n",
    "            for params in self.optimizer.param_groups:\n",
    "                params['lr'] = lr\n",
    "            if batch_num > num:\n",
    "                plt.plot(log_lrs[10:-5], losses[10:-5])\n",
    "                return log_lrs, losses    \n",
    "\n",
    "    def train(self, conf, epochs):\n",
    "        self.model.train()\n",
    "        \n",
    "        running_loss = 0.            \n",
    "        for e in range(epochs):\n",
    "            print('epoch {} started'.format(e))\n",
    "            \n",
    "            # schdule_lr: lr을 1/10으로 나누기\n",
    "            if e == self.milestones[0]:\n",
    "                self.schedule_lr()\n",
    "            if e == self.milestones[1]:\n",
    "                self.schedule_lr()      \n",
    "            if e == self.milestones[2]:\n",
    "                self.schedule_lr()    \n",
    "                \n",
    "            for imgs, labels in tqdm(iter(self.loader)):\n",
    "                imgs = imgs.to(conf.device)\n",
    "                labels = labels.to(conf.device)\n",
    "                self.optimizer.zero_grad()\n",
    "                embeddings = self.model(imgs)\n",
    "                thetas = self.head(embeddings, labels)\n",
    "                loss = conf.ce_loss(thetas, labels)\n",
    "                loss.backward()\n",
    "                running_loss += loss.item()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                print('self.board_loss_every',self.board_loss_every)\n",
    "                if self.step % self.board_loss_every == 0 and self.step != 0:\n",
    "                    loss_board = running_loss / self.board_loss_every\n",
    "                    self.writer.add_scalar('train_loss', loss_board, self.step)\n",
    "                    running_loss = 0.\n",
    "                \n",
    "                if self.step % self.evaluate_every == 0 and self.step != 0:\n",
    "#                     accuracy, best_threshold, roc_curve_tensor = self.evaluate(conf, self.agedb_30, self.agedb_30_issame)\n",
    "#                     self.board_val('agedb_30', accuracy, best_threshold, roc_curve_tensor)\n",
    "                    accuracy, best_threshold, roc_curve_tensor = self.evaluate(conf, self.lfw, self.lfw_issame)\n",
    "                    self.board_val('lfw', accuracy, best_threshold, roc_curve_tensor)\n",
    "#                     accuracy, best_threshold, roc_curve_tensor = self.evaluate(conf, self.cfp_fp, self.cfp_fp_issame)\n",
    "#                     self.board_val('cfp_fp', accuracy, best_threshold, roc_curve_tensor)\n",
    "                    self.model.train()\n",
    "                if self.step % self.save_every == 0 and self.step != 0:\n",
    "                    self.save_state(conf, accuracy)\n",
    "                    \n",
    "                self.step += 1\n",
    "                \n",
    "        self.save_state(conf, accuracy, to_save_folder=True, extra='final')\n",
    "\n",
    "    def schedule_lr(self):\n",
    "        print('self.optimizer.param_groups',self.optimizer.param_groups)\n",
    "        for params in self.optimizer.param_groups:                 \n",
    "            params['lr'] /= 10\n",
    "        print(self.optimizer)\n",
    "    \n",
    "    def infer(self, conf, faces, target_embs, tta=False):\n",
    "        '''\n",
    "        faces : list of PIL Image\n",
    "        target_embs : [n, 512] computed embeddings of faces in facebank\n",
    "        names : recorded names of faces in facebank\n",
    "        tta : test time augmentation (hfilp, that's all)\n",
    "        '''\n",
    "        embs = []\n",
    "        for img in faces:\n",
    "            if tta:\n",
    "                mirror = trans.functional.hflip(img)\n",
    "                emb = self.model(conf.test_transform(img).to(conf.device).unsqueeze(0))\n",
    "                emb_mirror = self.model(conf.test_transform(mirror).to(conf.device).unsqueeze(0))\n",
    "                embs.append(l2_norm(emb + emb_mirror))\n",
    "            else:                        \n",
    "                embs.append(self.model(conf.test_transform(img).to(conf.device).unsqueeze(0)))\n",
    "        source_embs = torch.cat(embs)\n",
    "        \n",
    "        diff = source_embs.unsqueeze(-1) - target_embs.transpose(1,0).unsqueeze(0)\n",
    "        dist = torch.sum(torch.pow(diff, 2), dim=1)\n",
    "        minimum, min_idx = torch.min(dist, dim=1)\n",
    "        min_idx[minimum > self.threshold] = -1 # if no match, set idx to -1\n",
    "        return min_idx, minimum               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기본 적인 변수들: config.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T14:09:49.326475Z",
     "start_time": "2020-07-21T14:09:49.308515Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_config(training = True):\n",
    "    conf = edict()\n",
    "    conf.data_path = Path('../data')\n",
    "    conf.work_path = Path('./work_space/')\n",
    "    conf.model_path = conf.work_path/'models'\n",
    "    conf.log_path = conf.work_path/'log'\n",
    "    conf.save_path = conf.work_path/'save'\n",
    "    conf.input_size = [112,112]\n",
    "    conf.embedding_size = 512\n",
    "    conf.use_mobilfacenet = False\n",
    "    conf.net_depth = 50\n",
    "    conf.drop_ratio = 0.6\n",
    "    conf.net_mode = 'ir_se' # or 'ir'\n",
    "    conf.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    conf.test_transform = trans.Compose([\n",
    "                    trans.ToTensor(),\n",
    "                    trans.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "                ])\n",
    "    conf.data_mode = 'emore'\n",
    "    conf.smallvgg_folder = conf.data_path/'small_vgg'\n",
    "    conf.vgg_folder = conf.data_path/'faces_vgg2_112x112'\n",
    "#     conf.ms1m_folder = conf.data_path/'faces_ms1m_112x112'\n",
    "#     conf.emore_folder = conf.data_path/'faces_emore'\n",
    "    conf.batch_size = 100 # irse net depth 50 \n",
    "#   conf.batch_size = 200 # mobilefacenet\n",
    "    conf.batch_size = 32 # small_vgg\n",
    "\n",
    "#--------------------Training Config ------------------------    \n",
    "    if training:        \n",
    "        conf.log_path = conf.work_path/'log'\n",
    "        conf.save_path = conf.work_path/'save'\n",
    "    #     conf.weight_decay = 5e-4\n",
    "        conf.lr = 1e-3\n",
    "        conf.milestones = [12,15,18]\n",
    "        conf.momentum = 0.9\n",
    "        conf.pin_memory = True\n",
    "#         conf.num_workers = 4 # when batchsize is 200\n",
    "        conf.num_workers = 3\n",
    "        conf.ce_loss = CrossEntropyLoss()    \n",
    "        conf.threshold = 1.5\n",
    "#--------------------Inference Config ------------------------\n",
    "    else:\n",
    "        conf.facebank_path = conf.data_path/'facebank'\n",
    "        conf.threshold = 1.5\n",
    "        conf.face_limit = 10 \n",
    "        #when inference, at maximum detect 10 faces in one image, my laptop is slow\n",
    "        conf.min_face_size = 30 \n",
    "        # the larger this value, the faster deduction, comes with tradeoff in small faces\n",
    "    return conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T14:09:49.694587Z",
     "start_time": "2020-07-21T14:09:49.682584Z"
    }
   },
   "outputs": [],
   "source": [
    "args = edict()\n",
    "\n",
    "args.epochs =20\n",
    "args.net_mode = 'ir_se'  #\"which network, [ir, ir_se, mobilefacenet]\"\n",
    "args.net_depth = 50 # \"how many layers [50,100,152]\"\n",
    "args.lr = 1e-3\n",
    "args.batch_size = 32\n",
    "args.num_workers = 3\n",
    "args.data_mode = 'small_vgg' # use which database, [vgg, ms1m, emore, concat]\"\n",
    "\n",
    "conf = get_config()\n",
    "\n",
    "if args.net_mode == 'mobilefacenet':\n",
    "        conf.use_mobilfacenet = True\n",
    "else:\n",
    "    conf.net_mode = args.net_mode\n",
    "    conf.net_depth = args.net_depth    \n",
    "\n",
    "    \n",
    "conf.lr = args.lr\n",
    "conf.batch_size = args.batch_size\n",
    "conf.num_workers = args.num_workers\n",
    "conf.data_mode = args.data_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T14:09:50.370800Z",
     "start_time": "2020-07-21T14:09:49.880772Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data_path': WindowsPath('../data'), 'work_path': WindowsPath('work_space'), 'model_path': WindowsPath('work_space/models'), 'log_path': WindowsPath('work_space/log'), 'save_path': WindowsPath('work_space/save'), 'input_size': [112, 112], 'embedding_size': 512, 'use_mobilfacenet': False, 'net_depth': 50, 'drop_ratio': 0.6, 'net_mode': 'ir_se', 'device': device(type='cpu'), 'test_transform': Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
      "), 'data_mode': 'small_vgg', 'smallvgg_folder': WindowsPath('../data/small_vgg'), 'vgg_folder': WindowsPath('../data/faces_vgg2_112x112'), 'batch_size': 32, 'lr': 0.001, 'milestones': [12, 15, 18], 'momentum': 0.9, 'pin_memory': True, 'num_workers': 3, 'ce_loss': CrossEntropyLoss(), 'threshold': 1.5}\n",
      "ir_se_50 model generated\n",
      "class_num 3\n",
      "self.loader <torch.utils.data.dataloader.DataLoader object at 0x0000013556A32E08>\n",
      "two model heads generated\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.001\n",
      "    momentum: 0.9\n",
      "    nesterov: False\n",
      "    weight_decay: 0.0005\n",
      ")\n",
      "optimizers generated\n"
     ]
    }
   ],
   "source": [
    "learner = face_learner(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T14:09:50.392797Z",
     "start_time": "2020-07-21T14:09:50.375796Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Backbone(\n",
       "  (input_layer): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): PReLU(num_parameters=64)\n",
       "  )\n",
       "  (output_layer): Sequential(\n",
       "    (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): Dropout(p=0.6, inplace=False)\n",
       "    (2): Flatten()\n",
       "    (3): Linear(in_features=25088, out_features=512, bias=True)\n",
       "    (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (body): Sequential(\n",
       "    (0): bottleneck_IR_SE(\n",
       "      (shortcut_layer): MaxPool2d(kernel_size=1, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (res_layer): Sequential(\n",
       "        (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (2): PReLU(num_parameters=64)\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (fc2): Conv2d(4, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): bottleneck_IR_SE(\n",
       "      (shortcut_layer): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "      (res_layer): Sequential(\n",
       "        (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (2): PReLU(num_parameters=64)\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (fc2): Conv2d(4, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): bottleneck_IR_SE(\n",
       "      (shortcut_layer): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "      (res_layer): Sequential(\n",
       "        (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (2): PReLU(num_parameters=64)\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (fc2): Conv2d(4, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): bottleneck_IR_SE(\n",
       "      (shortcut_layer): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (res_layer): Sequential(\n",
       "        (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (2): PReLU(num_parameters=128)\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (fc2): Conv2d(8, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): bottleneck_IR_SE(\n",
       "      (shortcut_layer): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "      (res_layer): Sequential(\n",
       "        (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (2): PReLU(num_parameters=128)\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (fc2): Conv2d(8, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): bottleneck_IR_SE(\n",
       "      (shortcut_layer): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "      (res_layer): Sequential(\n",
       "        (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (2): PReLU(num_parameters=128)\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (fc2): Conv2d(8, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): bottleneck_IR_SE(\n",
       "      (shortcut_layer): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "      (res_layer): Sequential(\n",
       "        (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (2): PReLU(num_parameters=128)\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (fc2): Conv2d(8, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): bottleneck_IR_SE(\n",
       "      (shortcut_layer): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (res_layer): Sequential(\n",
       "        (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (2): PReLU(num_parameters=256)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): bottleneck_IR_SE(\n",
       "      (shortcut_layer): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "      (res_layer): Sequential(\n",
       "        (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (2): PReLU(num_parameters=256)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): bottleneck_IR_SE(\n",
       "      (shortcut_layer): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "      (res_layer): Sequential(\n",
       "        (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (2): PReLU(num_parameters=256)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (10): bottleneck_IR_SE(\n",
       "      (shortcut_layer): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "      (res_layer): Sequential(\n",
       "        (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (2): PReLU(num_parameters=256)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (11): bottleneck_IR_SE(\n",
       "      (shortcut_layer): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "      (res_layer): Sequential(\n",
       "        (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (2): PReLU(num_parameters=256)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (12): bottleneck_IR_SE(\n",
       "      (shortcut_layer): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "      (res_layer): Sequential(\n",
       "        (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (2): PReLU(num_parameters=256)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (13): bottleneck_IR_SE(\n",
       "      (shortcut_layer): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "      (res_layer): Sequential(\n",
       "        (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (2): PReLU(num_parameters=256)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (14): bottleneck_IR_SE(\n",
       "      (shortcut_layer): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "      (res_layer): Sequential(\n",
       "        (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (2): PReLU(num_parameters=256)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (15): bottleneck_IR_SE(\n",
       "      (shortcut_layer): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "      (res_layer): Sequential(\n",
       "        (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (2): PReLU(num_parameters=256)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (16): bottleneck_IR_SE(\n",
       "      (shortcut_layer): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "      (res_layer): Sequential(\n",
       "        (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (2): PReLU(num_parameters=256)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (17): bottleneck_IR_SE(\n",
       "      (shortcut_layer): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "      (res_layer): Sequential(\n",
       "        (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (2): PReLU(num_parameters=256)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (18): bottleneck_IR_SE(\n",
       "      (shortcut_layer): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "      (res_layer): Sequential(\n",
       "        (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (2): PReLU(num_parameters=256)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (19): bottleneck_IR_SE(\n",
       "      (shortcut_layer): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "      (res_layer): Sequential(\n",
       "        (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (2): PReLU(num_parameters=256)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (20): bottleneck_IR_SE(\n",
       "      (shortcut_layer): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "      (res_layer): Sequential(\n",
       "        (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (2): PReLU(num_parameters=256)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (21): bottleneck_IR_SE(\n",
       "      (shortcut_layer): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (res_layer): Sequential(\n",
       "        (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (2): PReLU(num_parameters=512)\n",
       "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (22): bottleneck_IR_SE(\n",
       "      (shortcut_layer): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "      (res_layer): Sequential(\n",
       "        (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (2): PReLU(num_parameters=512)\n",
       "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (23): bottleneck_IR_SE(\n",
       "      (shortcut_layer): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "      (res_layer): Sequential(\n",
       "        (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (2): PReLU(num_parameters=512)\n",
       "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T14:10:21.902267Z",
     "start_time": "2020-07-21T14:09:51.221240Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/19 [00:25<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-b70990faebae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlearner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-34-4ae81f95b023>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, conf, epochs)\u001b[0m\n\u001b[0;32m    197\u001b[0m                 \u001b[0mthetas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mce_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthetas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mrunning_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shiney\\documents\\workspace\\pytorch\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m         \"\"\"\n\u001b[1;32m--> 198\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shiney\\documents\\workspace\\pytorch\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#learner.train(conf, args.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# learner.train분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T14:10:26.389818Z",
     "start_time": "2020-07-21T14:10:26.383818Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data_path': WindowsPath('../data'), 'work_path': WindowsPath('work_space'), 'model_path': WindowsPath('work_space/models'), 'log_path': WindowsPath('work_space/log'), 'save_path': WindowsPath('work_space/save'), 'input_size': [112, 112], 'embedding_size': 512, 'use_mobilfacenet': False, 'net_depth': 50, 'drop_ratio': 0.6, 'net_mode': 'ir_se', 'device': device(type='cpu'), 'test_transform': Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
      "), 'data_mode': 'small_vgg', 'smallvgg_folder': WindowsPath('../data/small_vgg'), 'vgg_folder': WindowsPath('../data/faces_vgg2_112x112'), 'batch_size': 32, 'lr': 0.001, 'milestones': [12, 15, 18], 'momentum': 0.9, 'pin_memory': True, 'num_workers': 3, 'ce_loss': CrossEntropyLoss(), 'threshold': 1.5}\n"
     ]
    }
   ],
   "source": [
    "inference=False\n",
    "step =0\n",
    "print(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T14:10:27.162816Z",
     "start_time": "2020-07-21T14:10:26.600816Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ir_se_50 model generated\n"
     ]
    }
   ],
   "source": [
    "if conf.use_mobilfacenet:\n",
    "    model = MobileFaceNet(conf.embedding_size).to(conf.device)\n",
    "    print('MobileFaceNet model generated')\n",
    "else:\n",
    "    model = Backbone(conf.net_depth, conf.drop_ratio, conf.net_mode).to(conf.device)\n",
    "    print('{}_{} model generated'.format(conf.net_mode, conf.net_depth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T14:10:27.195818Z",
     "start_time": "2020-07-21T14:10:27.165825Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_num 3\n",
      "loader <torch.utils.data.dataloader.DataLoader object at 0x00000135567D2248>\n",
      "two model heads generated\n"
     ]
    }
   ],
   "source": [
    "if not inference:\n",
    "    milestones = conf.milestones\n",
    "    loader, class_num = get_train_loader(conf)   \n",
    "    print('loader',loader)\n",
    "\n",
    "    writer = SummaryWriter(conf.log_path)\n",
    "    step = 0\n",
    "    head = Arcface(embedding_size=conf.embedding_size, classnum=class_num).to(conf.device)\n",
    "\n",
    "    print('two model heads generated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T14:10:27.214823Z",
     "start_time": "2020-07-21T14:10:27.198821Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108 635\n"
     ]
    }
   ],
   "source": [
    "paras_only_bn, paras_wo_bn = separate_bn_paras(model)\n",
    "print(len(paras_only_bn), len(paras_wo_bn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T14:10:27.280821Z",
     "start_time": "2020-07-21T14:10:27.268822Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.001\n",
      "    momentum: 0.9\n",
      "    nesterov: False\n",
      "    weight_decay: 0.0005\n",
      ")\n",
      "optimizers generated\n"
     ]
    }
   ],
   "source": [
    "if conf.use_mobilfacenet:\n",
    "    optimizer = optim.SGD([\n",
    "                        {'params': paras_wo_bn[:-1], 'weight_decay': 4e-5},\n",
    "                        {'params': [paras_wo_bn[-1]] + [head.kernel], 'weight_decay': 4e-4},\n",
    "                        {'params': paras_only_bn}\n",
    "                    ], lr = conf.lr, momentum = conf.momentum)\n",
    "else:\n",
    "    optimizer = optim.SGD([\n",
    "                        {'params': paras_wo_bn + [head.kernel], 'weight_decay': 5e-4},\n",
    "#                                     {'params': paras_only_bn}\n",
    "                    ], lr = conf.lr, momentum = conf.momentum)\n",
    "print(optimizer)\n",
    "#             self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, patience=40, verbose=True)\n",
    "\n",
    "print('optimizers generated')    \n",
    "#             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T14:10:27.503331Z",
     "start_time": "2020-07-21T14:10:27.488331Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Backbone(\n",
       "  (input_layer): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): PReLU(num_parameters=64)\n",
       "  )\n",
       "  (output_layer): Sequential(\n",
       "    (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): Dropout(p=0.6, inplace=False)\n",
       "    (2): Flatten()\n",
       "    (3): Linear(in_features=25088, out_features=512, bias=True)\n",
       "    (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (body): Sequential(\n",
       "    (0): bottleneck_IR_SE(\n",
       "      (shortcut_layer): MaxPool2d(kernel_size=1, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (res_layer): Sequential(\n",
       "        (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (2): PReLU(num_parameters=64)\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (fc2): Conv2d(4, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): bottleneck_IR_SE(\n",
       "      (shortcut_layer): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "      (res_layer): Sequential(\n",
       "        (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (2): PReLU(num_parameters=64)\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (fc2): Conv2d(4, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): bottleneck_IR_SE(\n",
       "      (shortcut_layer): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "      (res_layer): Sequential(\n",
       "        (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (2): PReLU(num_parameters=64)\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (fc2): Conv2d(4, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): bottleneck_IR_SE(\n",
       "      (shortcut_layer): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (res_layer): Sequential(\n",
       "        (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (2): PReLU(num_parameters=128)\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (fc2): Conv2d(8, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): bottleneck_IR_SE(\n",
       "      (shortcut_layer): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "      (res_layer): Sequential(\n",
       "        (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (2): PReLU(num_parameters=128)\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (fc2): Conv2d(8, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): bottleneck_IR_SE(\n",
       "      (shortcut_layer): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "      (res_layer): Sequential(\n",
       "        (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (2): PReLU(num_parameters=128)\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (fc2): Conv2d(8, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): bottleneck_IR_SE(\n",
       "      (shortcut_layer): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "      (res_layer): Sequential(\n",
       "        (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (2): PReLU(num_parameters=128)\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (fc2): Conv2d(8, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): bottleneck_IR_SE(\n",
       "      (shortcut_layer): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (res_layer): Sequential(\n",
       "        (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (2): PReLU(num_parameters=256)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): bottleneck_IR_SE(\n",
       "      (shortcut_layer): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "      (res_layer): Sequential(\n",
       "        (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (2): PReLU(num_parameters=256)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): bottleneck_IR_SE(\n",
       "      (shortcut_layer): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "      (res_layer): Sequential(\n",
       "        (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (2): PReLU(num_parameters=256)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (10): bottleneck_IR_SE(\n",
       "      (shortcut_layer): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "      (res_layer): Sequential(\n",
       "        (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (2): PReLU(num_parameters=256)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (11): bottleneck_IR_SE(\n",
       "      (shortcut_layer): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "      (res_layer): Sequential(\n",
       "        (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (2): PReLU(num_parameters=256)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (12): bottleneck_IR_SE(\n",
       "      (shortcut_layer): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "      (res_layer): Sequential(\n",
       "        (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (2): PReLU(num_parameters=256)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (13): bottleneck_IR_SE(\n",
       "      (shortcut_layer): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "      (res_layer): Sequential(\n",
       "        (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (2): PReLU(num_parameters=256)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (14): bottleneck_IR_SE(\n",
       "      (shortcut_layer): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "      (res_layer): Sequential(\n",
       "        (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (2): PReLU(num_parameters=256)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (15): bottleneck_IR_SE(\n",
       "      (shortcut_layer): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "      (res_layer): Sequential(\n",
       "        (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (2): PReLU(num_parameters=256)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (16): bottleneck_IR_SE(\n",
       "      (shortcut_layer): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "      (res_layer): Sequential(\n",
       "        (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (2): PReLU(num_parameters=256)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (17): bottleneck_IR_SE(\n",
       "      (shortcut_layer): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "      (res_layer): Sequential(\n",
       "        (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (2): PReLU(num_parameters=256)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (18): bottleneck_IR_SE(\n",
       "      (shortcut_layer): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "      (res_layer): Sequential(\n",
       "        (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (2): PReLU(num_parameters=256)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (19): bottleneck_IR_SE(\n",
       "      (shortcut_layer): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "      (res_layer): Sequential(\n",
       "        (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (2): PReLU(num_parameters=256)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (20): bottleneck_IR_SE(\n",
       "      (shortcut_layer): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "      (res_layer): Sequential(\n",
       "        (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (2): PReLU(num_parameters=256)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (21): bottleneck_IR_SE(\n",
       "      (shortcut_layer): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (res_layer): Sequential(\n",
       "        (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (2): PReLU(num_parameters=512)\n",
       "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (22): bottleneck_IR_SE(\n",
       "      (shortcut_layer): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "      (res_layer): Sequential(\n",
       "        (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (2): PReLU(num_parameters=512)\n",
       "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (23): bottleneck_IR_SE(\n",
       "      (shortcut_layer): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "      (res_layer): Sequential(\n",
       "        (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (2): PReLU(num_parameters=512)\n",
       "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T14:10:27.722330Z",
     "start_time": "2020-07-21T14:10:27.703365Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "(198, 3, 112, 112) 99\n",
      "1.5\n"
     ]
    }
   ],
   "source": [
    "print(len(loader))  # 데이터를 32개씩 7번 갖고오기\n",
    "# self.board_loss_every = len(self.loader)//100\n",
    "# self.evaluate_every = len(self.loader)//10\n",
    "# self.save_every = len(self.loader)//5\n",
    "board_loss_every = len(loader)\n",
    "evaluate_every = len(loader)\n",
    "save_every = len(loader)\n",
    "#             self.agedb_30, self.cfp_fp, self.lfw, self.agedb_30_issame, self.cfp_fp_issame, self.lfw_issame = get_val_data(self.loader.dataset.root.parent)\n",
    "lfw, lfw_issame = get_val_data(conf.smallvgg_folder)\n",
    "print(lfw.shape, len(lfw_issame))\n",
    "\n",
    "#         else:\n",
    "threshold = conf.threshold\n",
    "print(threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T14:10:27.934341Z",
     "start_time": "2020-07-21T14:10:27.916365Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_state(conf, accuracy, to_save_folder=False, extra=None, model_only=False):\n",
    "    if to_save_folder:\n",
    "        save_path = conf.save_path\n",
    "    else:\n",
    "        save_path = conf.model_path\n",
    "    torch.save(\n",
    "        model.state_dict(), save_path('model_{}_accuracy:{}_step:{}_{}.pth'.format(get_time(), accuracy, step, extra)))\n",
    "    if not model_only:\n",
    "        torch.save(\n",
    "            head.state_dict(), save_path('head_{}_accuracy:{}_step:{}_{}.pth'.format(get_time(), accuracy, step, extra)))\n",
    "        torch.save(\n",
    "            optimizer.state_dict(), save_path('optimizer_{}_accuracy:{}_step:{}_{}.pth'.format(get_time(), accuracy, step, extra)))\n",
    "\n",
    "def load_state(conf, fixed_str, from_save_folder=False, model_only=False):\n",
    "    if from_save_folder:\n",
    "        save_path = conf.save_path\n",
    "    else:\n",
    "        save_path = conf.model_path            \n",
    "    model.load_state_dict(torch.load(save_path/'model_{}'.format(fixed_str)))\n",
    "    if not model_only:\n",
    "        head.load_state_dict(torch.load(save_path/'head_{}'.format(fixed_str)))\n",
    "        optimizer.load_state_dict(torch.load(save_path/'optimizer_{}'.format(fixed_str)))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T14:10:28.800974Z",
     "start_time": "2020-07-21T14:10:28.775003Z"
    }
   },
   "outputs": [],
   "source": [
    "def board_val(db_name, accuracy, best_threshold, roc_curve_tensor, step):\n",
    "    \"\"\"log 남기기\"\"\"\n",
    "    writer.add_scalar('{}_accuracy'.format(db_name), accuracy, step)\n",
    "    writer.add_scalar('{}_best_threshold'.format(db_name), best_threshold, step)\n",
    "    writer.add_image('{}_roc_curve'.format(db_name), roc_curve_tensor, step)\n",
    "#         self.writer.add_scalar('{}_val:true accept ratio'.format(db_name), val, self.step)\n",
    "#         self.writer.add_scalar('{}_val_std'.format(db_name), val_std, self.step)\n",
    "#         self.writer.add_scalar('{}_far:False Acceptance Ratio'.format(db_name), far, self.step)\n",
    "\n",
    "\n",
    "\n",
    "def find_lr(\n",
    "            conf,\n",
    "            init_value=1e-8,\n",
    "            final_value=10.,\n",
    "            beta=0.98,\n",
    "            bloding_scale=3.,\n",
    "            num=None):\n",
    "    if not num:\n",
    "        num = len(self.loader)\n",
    "    mult = (final_value / init_value)**(1 / num)\n",
    "    lr = init_value\n",
    "    for params in optimizer.param_groups:\n",
    "        params['lr'] = lr\n",
    "    model.train()\n",
    "    avg_loss = 0.\n",
    "    best_loss = 0.\n",
    "    batch_num = 0\n",
    "    losses = []\n",
    "    log_lrs = []\n",
    "    for i, (imgs, labels) in tqdm(enumerate(loader), total=num):\n",
    "\n",
    "        imgs = imgs.to(conf.device)\n",
    "        labels = labels.to(conf.device)\n",
    "        batch_num += 1          \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        embeddings = model(imgs)\n",
    "        thetas = head(embeddings, labels)\n",
    "        loss = conf.ce_loss(thetas, labels)          \n",
    "\n",
    "        #Compute the smoothed loss\n",
    "        avg_loss = beta * avg_loss + (1 - beta) * loss.item()\n",
    "        writer.add_scalar('avg_loss', avg_loss, batch_num)\n",
    "        smoothed_loss = avg_loss / (1 - beta**batch_num)\n",
    "        writer.add_scalar('smoothed_loss', smoothed_loss,batch_num)\n",
    "        #Stop if the loss is exploding\n",
    "        if batch_num > 1 and smoothed_loss > bloding_scale * best_loss:\n",
    "            print('exited with best_loss at {}'.format(best_loss))\n",
    "            plt.plot(log_lrs[10:-5], losses[10:-5])\n",
    "            return log_lrs, losses\n",
    "        #Record the best loss\n",
    "        if smoothed_loss < best_loss or batch_num == 1:\n",
    "            best_loss = smoothed_loss\n",
    "        #Store the values\n",
    "        losses.append(smoothed_loss)\n",
    "        log_lrs.append(math.log10(lr))\n",
    "        writer.add_scalar('log_lr', math.log10(lr), batch_num)\n",
    "        #Do the SGD step\n",
    "        #Update the lr for the next step\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        lr *= mult\n",
    "        for params in optimizer.param_groups:\n",
    "            params['lr'] = lr\n",
    "        if batch_num > num:\n",
    "            plt.plot(log_lrs[10:-5], losses[10:-5])\n",
    "            return log_lrs, losses    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T14:10:29.032978Z",
     "start_time": "2020-07-21T14:10:29.023971Z"
    }
   },
   "outputs": [],
   "source": [
    "def schedule_lr():\n",
    "    print('self.optimizer.param_groups',optimizer.param_groups)\n",
    "    for params in optimizer.param_groups:                 \n",
    "        params['lr'] /= 10\n",
    "    print(optimizer)\n",
    "\n",
    "def infer( conf, faces, target_embs, tta=False):\n",
    "    '''\n",
    "    faces : list of PIL Image\n",
    "    target_embs : [n, 512] computed embeddings of faces in facebank\n",
    "    names : recorded names of faces in facebank\n",
    "    tta : test time augmentation (hfilp, that's all)\n",
    "    '''\n",
    "    embs = []\n",
    "    for img in faces:\n",
    "        if tta:\n",
    "            mirror = trans.functional.hflip(img)\n",
    "            emb = model(conf.test_transform(img).to(conf.device).unsqueeze(0))\n",
    "            emb_mirror = self.model(conf.test_transform(mirror).to(conf.device).unsqueeze(0))\n",
    "            embs.append(l2_norm(emb + emb_mirror))\n",
    "        else:                        \n",
    "            embs.append(model(conf.test_transform(img).to(conf.device).unsqueeze(0)))\n",
    "    source_embs = torch.cat(embs)\n",
    "\n",
    "    diff = source_embs.unsqueeze(-1) - target_embs.transpose(1,0).unsqueeze(0)\n",
    "    dist = torch.sum(torch.pow(diff, 2), dim=1)\n",
    "    minimum, min_idx = torch.min(dist, dim=1)\n",
    "    min_idx[minimum > self.threshold] = -1 # if no match, set idx to -1\n",
    "    return min_idx, minimum               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  self_evaluate(conf,lfw, lfw_issame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T14:10:29.748173Z",
     "start_time": "2020-07-21T14:10:29.727172Z"
    }
   },
   "outputs": [],
   "source": [
    "def self_evaluate(conf, carray, issame, nrof_folds = 5, tta = False):\n",
    "    print('*******start self_evaluate*******')\n",
    "    model.eval()\n",
    "    idx = 0\n",
    "    embeddings = np.zeros([len(carray), conf.embedding_size])\n",
    "    print('embeddings.shape', embeddings.shape)\n",
    "    print('len(carray)', len(carray))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "       \n",
    "        while idx + conf.batch_size <= len(carray):\n",
    "            print('idx', idx)\n",
    "            print('idx + conf.batch_size', idx + conf.batch_size)\n",
    "            batch = torch.tensor(carray[idx:idx + conf.batch_size])\n",
    "            print('-batch.shape', batch.shape)\n",
    "            \n",
    "             # tta : test time augmentation (hfilp, that's all)\n",
    "            if tta:\n",
    "                fliped = hflip_batch(batch)\n",
    "                emb_batch = model(batch.to(conf.device)) + model(fliped.to(conf.device))\n",
    "                embeddings[idx:idx + conf.batch_size] = l2_norm(emb_batch)\n",
    "            else:\n",
    "                print('-model(batch.to(conf.device)).cpu().shape', model(batch.to(conf.device)).cpu().shape)\n",
    "                embeddings[idx:idx + conf.batch_size] = model(batch.to(conf.device)).cpu()\n",
    "            idx += conf.batch_size\n",
    "            \n",
    "        if idx < len(carray):\n",
    "            batch = torch.tensor(carray[idx:])            \n",
    "            if tta:\n",
    "                fliped = hflip_batch(batch)\n",
    "                emb_batch = model(batch.to(conf.device)) + model(fliped.to(conf.device))\n",
    "                embeddings[idx:] = l2_norm(emb_batch)\n",
    "            else:\n",
    "                embeddings[idx:] = model(batch.to(conf.device)).cpu()\n",
    "    tpr, fpr, accuracy, best_thresholds = evaluate(embeddings, issame, nrof_folds)\n",
    "    print('tpr',tpr) \n",
    "    print('fpr',fpr)\n",
    "    print('accuracy',accuracy), \n",
    "    print('best_thresholds', best_thresholds)\n",
    "    buf = gen_plot(fpr, tpr)\n",
    "    roc_curve = Image.open(buf)\n",
    "    roc_curve_tensor = trans.ToTensor()(roc_curve)\n",
    "    return accuracy.mean(), best_thresholds.mean(), roc_curve_tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### evaluate(embeddings, issame, nrof_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T14:10:30.762180Z",
     "start_time": "2020-07-21T14:10:30.752174Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(embeddings, actual_issame, nrof_folds=10,pca=0):\n",
    "    print('**********evaluate start*******')\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    print('embeddings.shape',embeddings.shape)\n",
    "    thresholds = np.arange(0, 4, 0.01)\n",
    "    embeddings1 = embeddings[0::2]\n",
    "    embeddings2 = embeddings[1::2]\n",
    "    tpr, fpr, accuracy, best_thresholds = calculate_roc(thresholds, embeddings1, embeddings2,\\\n",
    "                                       np.asarray(actual_issame), nrof_folds=nrof_folds, pca=pca)\n",
    "    #     thresholds = np.arange(0, 4, 0.001)\n",
    "    #     val, val_std, far = calculate_val(thresholds, embeddings1, embeddings2,\n",
    "    #                                       np.asarray(actual_issame), 1e-3, nrof_folds=nrof_folds)\n",
    "    #     return tpr, fpr, accuracy, best_thresholds, val, val_std, far\n",
    "    return tpr, fpr, accuracy, best_thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### calculate_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T14:16:22.012493Z",
     "start_time": "2020-07-21T14:16:21.998497Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(threshold, dist, actual_issame):\n",
    "#     print('***********calucluate_accuracy start***********')\n",
    "    predict_issame = np.less(dist, threshold)\n",
    "    tp = np.sum(np.logical_and(predict_issame, actual_issame))\n",
    "    fp = np.sum(np.logical_and(predict_issame, np.logical_not(actual_issame)))\n",
    "    tn = np.sum(np.logical_and(np.logical_not(predict_issame), np.logical_not(actual_issame)))\n",
    "    fn = np.sum(np.logical_and(np.logical_not(predict_issame), actual_issame))\n",
    "\n",
    "    tpr = 0 if (tp + fn == 0) else float(tp) / float(tp + fn)\n",
    "    fpr = 0 if (fp + tn == 0) else float(fp) / float(fp + tn)\n",
    "    acc = float(tp + tn) / dist.size\n",
    "    return tpr, fpr, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### calculate_roc(thresholds, embeddings1, embeddings2,np.asarray(actual_issame), nrof_folds=nrof_folds, pca=pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T14:16:23.667491Z",
     "start_time": "2020-07-21T14:16:22.904499Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_roc(thresholds, embeddings1, embeddings2, actual_issame, nrof_folds=10, pca=0):\n",
    "    print('*********calculate_roc start**********')\n",
    "    print('thresholds',thresholds)\n",
    "    assert (embeddings1.shape[0] == embeddings2.shape[0])\n",
    "    assert (embeddings1.shape[1] == embeddings2.shape[1])\n",
    "    nrof_pairs = min(len(actual_issame), embeddings1.shape[0])\n",
    "    print('nrof_pairs',nrof_pairs)\n",
    "    nrof_thresholds = len(thresholds)\n",
    "    k_fold = KFold(n_splits=nrof_folds, shuffle=False)\n",
    "\n",
    "    tprs = np.zeros((nrof_folds, nrof_thresholds))\n",
    "    fprs = np.zeros((nrof_folds, nrof_thresholds))\n",
    "    accuracy = np.zeros((nrof_folds))\n",
    "    best_thresholds = np.zeros((nrof_folds))\n",
    "    indices = np.arange(nrof_pairs)\n",
    "    # print('pca', pca)\n",
    "\n",
    "    if pca == 0:\n",
    "        diff = np.subtract(embeddings1, embeddings2)\n",
    "        dist = np.sum(np.square(diff), 1)\n",
    "\n",
    "    for fold_idx, (train_set, test_set) in enumerate(k_fold.split(indices)):\n",
    "        # print('train_set', train_set)\n",
    "        # print('test_set', test_set)\n",
    "        if pca > 0:\n",
    "            print('doing pca on', fold_idx)\n",
    "            embed1_train = embeddings1[train_set]\n",
    "            embed2_train = embeddings2[train_set]\n",
    "            _embed_train = np.concatenate((embed1_train, embed2_train), axis=0)\n",
    "            # print(_embed_train.shape)\n",
    "            pca_model = PCA(n_components=pca)\n",
    "            pca_model.fit(_embed_train)\n",
    "            embed1 = pca_model.transform(embeddings1)\n",
    "            embed2 = pca_model.transform(embeddings2)\n",
    "            embed1 = sklearn.preprocessing.normalize(embed1)\n",
    "            embed2 = sklearn.preprocessing.normalize(embed2)\n",
    "            # print(embed1.shape, embed2.shape)\n",
    "            diff = np.subtract(embed1, embed2)\n",
    "            dist = np.sum(np.square(diff), 1)\n",
    "\n",
    "        # Find the best threshold for the fold\n",
    "        acc_train = np.zeros((nrof_thresholds))\n",
    "        for threshold_idx, threshold in enumerate(thresholds):\n",
    "            print(threshold_idx)\n",
    "            _, _, acc_train[threshold_idx] = calculate_accuracy(threshold, dist[train_set], actual_issame[train_set])\n",
    "       \n",
    "        \n",
    "        best_threshold_index = np.argmax(acc_train)\n",
    "#         print('best_threshold_index', best_threshold_index, acc_train[best_threshold_index])\n",
    "        best_thresholds[fold_idx] = thresholds[best_threshold_index]\n",
    "    \n",
    "    \n",
    "    \n",
    "        for threshold_idx, threshold in enumerate(thresholds):\n",
    "            tprs[fold_idx, threshold_idx], fprs[fold_idx, threshold_idx], _ = calculate_accuracy(threshold,\n",
    "                                                                                                 dist[test_set],\n",
    "                                                                                                 actual_issame[\n",
    "                                                                                                     test_set])\n",
    "        _, _, accuracy[fold_idx] = calculate_accuracy(thresholds[best_threshold_index], dist[test_set],\n",
    "                                                      actual_issame[test_set])\n",
    "\n",
    "    tpr = np.mean(tprs, 0)\n",
    "    fpr = np.mean(fprs, 0)\n",
    "    return tpr, fpr, accuracy, best_thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T14:16:24.058502Z",
     "start_time": "2020-07-21T14:16:24.033498Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(conf, epochs) :\n",
    "    epochs = 1\n",
    "    step = 1\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0.  \n",
    "    for e in range(epochs):\n",
    "        print('epoch {} started'.format(e))\n",
    "\n",
    "        # schdule_lr: lr을 1/10으로 나누기\n",
    "        if e == milestones[0]:\n",
    "            schedule_lr()\n",
    "        if e == milestones[1]:\n",
    "            schedule_lr()      \n",
    "        if e == milestones[2]:\n",
    "            schedule_lr()    \n",
    "\n",
    "\n",
    "        for imgs, labels in tqdm(iter(loader)):\n",
    "            print(step,'step training start!!!')\n",
    "            imgs = imgs.to(conf.device)\n",
    "    #         print('len(imgs)',len(imgs))\n",
    "            labels = labels.to(conf.device)\n",
    "    #         print('labels[0]',labels[0])\n",
    "            optimizer.zero_grad()\n",
    "            embeddings = model(imgs)\n",
    "    #         print('embeddings.shape',embeddings.shape)\n",
    "            thetas = head(embeddings, labels)\n",
    "            loss = conf.ce_loss(thetas, labels)\n",
    "            loss.backward()\n",
    "            running_loss += loss.item()\n",
    "            optimizer.step()\n",
    "    #             if step % board_loss_every == 0 and step != 0:\n",
    "            loss_board = running_loss / board_loss_every\n",
    "            writer.add_scalar('train_loss', loss_board, step)\n",
    "            running_loss = 0.\n",
    "\n",
    "            print('step', step)\n",
    "            print('ebaluate_every', evaluate_every)\n",
    "            print('save_every', save_every)\n",
    "            print('board_loss_every',board_loss_every)\n",
    "\n",
    "            if step % board_loss_every == 1 and step != 0:\n",
    "                loss_board = running_loss / board_loss_every\n",
    "                print('loss_board',loss_board)\n",
    "                writer.add_scalar('train_loss', loss_board, step)\n",
    "                running_loss = 0.\n",
    "\n",
    "\n",
    "            if step % evaluate_every == 1 and step != 0:\n",
    "    #             accuracy, best_threshold, roc_curve_tensor = self.evaluate(conf, self.agedb_30, self.agedb_30_issame)\n",
    "    #             self.board_val('agedb_30', accuracy, best_threshold, roc_curve_tensor)\n",
    "                accuracy, best_threshold, roc_curve_tensor = self_evaluate(conf,lfw, lfw_issame)\n",
    "\n",
    "                print('accuracy:', accuracy)\n",
    "                print('best_threshold', best_threshold)\n",
    "                print('roc_curve_tensor',roc_curve_tensor)\n",
    "                board_val('small_lfw', accuracy, best_threshold, roc_curve_tensor)\n",
    "    #             accuracy, best_threshold, roc_curve_tensor = self.evaluate(conf, self.cfp_fp, self.cfp_fp_issame)\n",
    "    #             self.board_val('cfp_fp', accuracy, best_threshold, roc_curve_tensor)\n",
    "                model.train()\n",
    "            if step % save_everly == 1 and step != 0:\n",
    "                save_state(conf, accuracy)\n",
    "\n",
    "            step += 1\n",
    "\n",
    "    save_state(conf, accuracy, to_save_folder=True, extra='final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-21T14:18:38.585707Z",
     "start_time": "2020-07-21T14:16:24.966492Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/19 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 step training start!!!\n",
      "step 1\n",
      "ebaluate_every 19\n",
      "save_every 19\n",
      "board_loss_every 19\n",
      "loss_board 0.0\n",
      "*******start self_evaluate*******\n",
      "embeddings.shape (198, 512)\n",
      "len(carray) 198\n",
      "idx 0\n",
      "idx + conf.batch_size 32\n",
      "-batch.shape torch.Size([32, 3, 112, 112])\n",
      "-model(batch.to(conf.device)).cpu().shape torch.Size([32, 512])\n",
      "idx 32\n",
      "idx + conf.batch_size 64\n",
      "-batch.shape torch.Size([32, 3, 112, 112])\n",
      "-model(batch.to(conf.device)).cpu().shape torch.Size([32, 512])\n",
      "idx 64\n",
      "idx + conf.batch_size 96\n",
      "-batch.shape torch.Size([32, 3, 112, 112])\n",
      "-model(batch.to(conf.device)).cpu().shape torch.Size([32, 512])\n",
      "idx 96\n",
      "idx + conf.batch_size 128\n",
      "-batch.shape torch.Size([32, 3, 112, 112])\n",
      "-model(batch.to(conf.device)).cpu().shape torch.Size([32, 512])\n",
      "idx 128\n",
      "idx + conf.batch_size 160\n",
      "-batch.shape torch.Size([32, 3, 112, 112])\n",
      "-model(batch.to(conf.device)).cpu().shape torch.Size([32, 512])\n",
      "idx 160\n",
      "idx + conf.batch_size 192\n",
      "-batch.shape torch.Size([32, 3, 112, 112])\n",
      "-model(batch.to(conf.device)).cpu().shape torch.Size([32, 512])\n",
      "**********evaluate start*******\n",
      "embeddings.shape (198, 512)\n",
      "*********calculate_roc start**********\n",
      "thresholds [0.   0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1  0.11 0.12 0.13\n",
      " 0.14 0.15 0.16 0.17 0.18 0.19 0.2  0.21 0.22 0.23 0.24 0.25 0.26 0.27\n",
      " 0.28 0.29 0.3  0.31 0.32 0.33 0.34 0.35 0.36 0.37 0.38 0.39 0.4  0.41\n",
      " 0.42 0.43 0.44 0.45 0.46 0.47 0.48 0.49 0.5  0.51 0.52 0.53 0.54 0.55\n",
      " 0.56 0.57 0.58 0.59 0.6  0.61 0.62 0.63 0.64 0.65 0.66 0.67 0.68 0.69\n",
      " 0.7  0.71 0.72 0.73 0.74 0.75 0.76 0.77 0.78 0.79 0.8  0.81 0.82 0.83\n",
      " 0.84 0.85 0.86 0.87 0.88 0.89 0.9  0.91 0.92 0.93 0.94 0.95 0.96 0.97\n",
      " 0.98 0.99 1.   1.01 1.02 1.03 1.04 1.05 1.06 1.07 1.08 1.09 1.1  1.11\n",
      " 1.12 1.13 1.14 1.15 1.16 1.17 1.18 1.19 1.2  1.21 1.22 1.23 1.24 1.25\n",
      " 1.26 1.27 1.28 1.29 1.3  1.31 1.32 1.33 1.34 1.35 1.36 1.37 1.38 1.39\n",
      " 1.4  1.41 1.42 1.43 1.44 1.45 1.46 1.47 1.48 1.49 1.5  1.51 1.52 1.53\n",
      " 1.54 1.55 1.56 1.57 1.58 1.59 1.6  1.61 1.62 1.63 1.64 1.65 1.66 1.67\n",
      " 1.68 1.69 1.7  1.71 1.72 1.73 1.74 1.75 1.76 1.77 1.78 1.79 1.8  1.81\n",
      " 1.82 1.83 1.84 1.85 1.86 1.87 1.88 1.89 1.9  1.91 1.92 1.93 1.94 1.95\n",
      " 1.96 1.97 1.98 1.99 2.   2.01 2.02 2.03 2.04 2.05 2.06 2.07 2.08 2.09\n",
      " 2.1  2.11 2.12 2.13 2.14 2.15 2.16 2.17 2.18 2.19 2.2  2.21 2.22 2.23\n",
      " 2.24 2.25 2.26 2.27 2.28 2.29 2.3  2.31 2.32 2.33 2.34 2.35 2.36 2.37\n",
      " 2.38 2.39 2.4  2.41 2.42 2.43 2.44 2.45 2.46 2.47 2.48 2.49 2.5  2.51\n",
      " 2.52 2.53 2.54 2.55 2.56 2.57 2.58 2.59 2.6  2.61 2.62 2.63 2.64 2.65\n",
      " 2.66 2.67 2.68 2.69 2.7  2.71 2.72 2.73 2.74 2.75 2.76 2.77 2.78 2.79\n",
      " 2.8  2.81 2.82 2.83 2.84 2.85 2.86 2.87 2.88 2.89 2.9  2.91 2.92 2.93\n",
      " 2.94 2.95 2.96 2.97 2.98 2.99 3.   3.01 3.02 3.03 3.04 3.05 3.06 3.07\n",
      " 3.08 3.09 3.1  3.11 3.12 3.13 3.14 3.15 3.16 3.17 3.18 3.19 3.2  3.21\n",
      " 3.22 3.23 3.24 3.25 3.26 3.27 3.28 3.29 3.3  3.31 3.32 3.33 3.34 3.35\n",
      " 3.36 3.37 3.38 3.39 3.4  3.41 3.42 3.43 3.44 3.45 3.46 3.47 3.48 3.49\n",
      " 3.5  3.51 3.52 3.53 3.54 3.55 3.56 3.57 3.58 3.59 3.6  3.61 3.62 3.63\n",
      " 3.64 3.65 3.66 3.67 3.68 3.69 3.7  3.71 3.72 3.73 3.74 3.75 3.76 3.77\n",
      " 3.78 3.79 3.8  3.81 3.82 3.83 3.84 3.85 3.86 3.87 3.88 3.89 3.9  3.91\n",
      " 3.92 3.93 3.94 3.95 3.96 3.97 3.98 3.99]\n",
      "nrof_pairs 99\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "tpr [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.02       0.03\n",
      " 0.03       0.03       0.03       0.03       0.04       0.04\n",
      " 0.04       0.04       0.04       0.05       0.05       0.05\n",
      " 0.05       0.06       0.09       0.11       0.11       0.13\n",
      " 0.14       0.14       0.14       0.16       0.16       0.17\n",
      " 0.19       0.23052632 0.25052632 0.27157895 0.27157895 0.27157895\n",
      " 0.28210526 0.29263158 0.30263158 0.32315789 0.33315789 0.34315789\n",
      " 0.36315789 0.36315789 0.36315789 0.39368421 0.40421053 0.41421053\n",
      " 0.42421053 0.44421053 0.44421053 0.44421053 0.45473684 0.48526316\n",
      " 0.49526316 0.49526316 0.50526316 0.50526316 0.52578947 0.54578947\n",
      " 0.55578947 0.56578947 0.58578947 0.60631579 0.61631579 0.62631579\n",
      " 0.65631579 0.66631579 0.68631579 0.68631579 0.69631579 0.69631579\n",
      " 0.71631579 0.71631579 0.71631579 0.73684211 0.74684211 0.76736842\n",
      " 0.76736842 0.76736842 0.76736842 0.76736842 0.77736842 0.79736842\n",
      " 0.81789474 0.81789474 0.82789474 0.82789474 0.82789474 0.82789474\n",
      " 0.82789474 0.82789474 0.82789474 0.83842105 0.83842105 0.83842105\n",
      " 0.83842105 0.83842105 0.84842105 0.84842105 0.84842105 0.84842105\n",
      " 0.85842105 0.87842105 0.87842105 0.87842105 0.87842105 0.87842105\n",
      " 0.87842105 0.87842105 0.88894737 0.88894737 0.88894737 0.90894737\n",
      " 0.91947368 0.91947368 0.92947368 0.92947368 0.93947368 0.93947368\n",
      " 0.93947368 0.93947368 0.93947368 0.93947368 0.93947368 0.93947368\n",
      " 0.93947368 0.93947368 0.94947368 0.94947368 0.94947368 0.95947368\n",
      " 0.95947368 0.95947368 0.96947368 0.96947368 0.96947368 0.96947368\n",
      " 0.97947368 0.97947368 0.97947368 0.97947368 0.97947368 0.97947368\n",
      " 0.97947368 0.97947368 0.97947368 0.97947368 0.97947368 0.97947368\n",
      " 0.97947368 0.97947368 0.97947368 0.97947368 0.98947368 0.98947368\n",
      " 0.98947368 0.98947368 0.98947368 0.98947368 0.98947368 0.98947368\n",
      " 0.98947368 1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.        ]\n",
      "fpr [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "accuracy [1.         1.         1.         1.         0.94736842]\n",
      "best_thresholds [1.75 1.75 1.75 1.75 1.66]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/19 [02:08<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'io' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-6887bb232763>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-58-3277ae78b5bb>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(conf, epochs)\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[1;31m#             accuracy, best_threshold, roc_curve_tensor = self.evaluate(conf, self.agedb_30, self.agedb_30_issame)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[1;31m#             self.board_val('agedb_30', accuracy, best_threshold, roc_curve_tensor)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m                 \u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_threshold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mroc_curve_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlfw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlfw_issame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'accuracy:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-50-2c5fdcbe5d93>\u001b[0m in \u001b[0;36mself_evaluate\u001b[1;34m(conf, carray, issame, nrof_folds, tta)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'best_thresholds'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_thresholds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m     \u001b[0mbuf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_plot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfpr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m     \u001b[0mroc_curve\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[0mroc_curve_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroc_curve\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-9749a9b8f555>\u001b[0m in \u001b[0;36mgen_plot\u001b[1;34m(fpr, tpr)\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ROC Curve\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m14\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0mplot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfpr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlinewidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m     \u001b[0mbuf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'jpeg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[0mbuf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'io' is not defined"
     ]
    }
   ],
   "source": [
    "train(conf, epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imgs dir 풀러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "231.094px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
